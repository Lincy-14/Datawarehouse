
# Data Warehouse

## Introduction

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
The aim of the project is to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.
The ETL pipeline is built for a database that was hosted on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## JSON Data in S3

**Song dataset*:** Song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

**Log Dataset:** It consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![logdata](log-data(1).png)

## Modules

1. Create tables  - create_tables.py  
2. ETL Pipeline   - etl.py

## Tables & Schema

### Fact table

1.songplays - records in log data associated with song plays i.e. records with page NextSong Columns

### Dimension table

1.users - users in the app

2.songs - songs in music database

3.artists - artists in music database

4.time - timestamps of records in songplays broken down into specific units

## ETL Pipeline 

The ETL pipeline will load the JSON files in the log_data directory on S3 into a staging table staging_events using a JSONPath file log_json_path.json to map the JSON elements to the columns in the staging_events table.

Next, the ETL pipeline will load the JSON files in the song_data directory on S3 into a staging table staging_songs.

Finally, the ETL pipeline will load the data from the staging tables and insert the appropriate data into the analytics tables: songplay, users, songs, artists and time.

## Repository Structure

1.create_tables.py : Python script to create the fact and dimension analytics tables on Redshift

2.etl.py : Python script to run the ETL pipeline that will load the data from S3 into staging tables on Redshift. It will then load the data from the staging tables to the analytics tables on Redshift

3.sql_queries.py : Script containing the SQL statements

4.dwh.cfg : Configuration file

## Execution steps

1.Create a cluster in Redshift
2.Fill in the details under [CLUSTER] and [IAM_ROLE] tags in the configuration file (dwh.cfg)
3.Execute create_tables.py to create connection with the respective database and Redshift cluster.In this step itself fact, dimension and staging tables will be created
4.Execute the etl.py to load the data in S3 to the staging tables followed by the data export from the staging tables to facts and dimension tables.
5.Query the data from the respective tables.

## Sample Queries

Here are some of the basic SQL queries to check the data in the analytics tables

select * from songplays limit 10; 
select * from songs limit 10;
select * from artists limit 10;
select * from time limit 10;
select * from users limit 10;

Results for the above queries are exported as csv files and stored under folder ***results***


